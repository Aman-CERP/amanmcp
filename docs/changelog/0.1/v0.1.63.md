# v0.1.63 - MLX Embedding Backend

**Release Date:** 2026-01-08

---

## Added

- **MLX-1**: MLX embedding backend for Apple Silicon (~55x faster than Ollama)
  - Add `MLXEmbedder` implementation with full Embedder interface support
  - Add `ProviderMLX` to embedder factory with auto-detection on Apple Silicon
  - Default provider now auto-detects: MLX (Apple Silicon) → Ollama → Static768
  - Add config.yaml support: `mlx_endpoint`, `mlx_model` fields
  - Add environment variable overrides: `AMANMCP_MLX_ENDPOINT`, `AMANMCP_MLX_MODEL`
  - Wire MLX config in index, serve, search, and daemon commands
  - Model sizes: small (0.6B, 1024 dims), medium (4B, 2560 dims), large (8B, 4096 dims)
  - Requires external MLX server (see: github.com/jakedahn/qwen3-embeddings-mlx)

## Documentation

- Major restructure of embedding optimization analysis for better usability
  - Add **Quick Start** guide: 3-step MLX setup for Apple Silicon
  - Add **Production Deployment**: macOS LaunchAgent for auto-start on login
  - Add **Backend Configuration**: MLX (recommended) vs Ollama (fallback)
  - Add **Troubleshooting Guide**: common issues and solutions
  - Move TEI content to **Appendix** (marked as historical/non-working)
  - Update **Implementation Plan** to focus on MLX backend integration
  - Update **Risk Assessment** to reflect MLX success

- Update embedding optimization analysis with TEI and MLX experimental validation
  - TEI v1.8.3 tested on M4 Pro with Metal - build succeeds but Qwen3 crashes
  - TEI + Metal works with nomic-embed-text (proof TEI/Metal itself is functional)
  - TEI + Qwen3 + Metal crashes during "Warming up model" (all sizes: 8B, 4B, 0.6B)
  - Document GitHub issues #630, #632, #663 showing incomplete Metal support
  - **MLX validation SUCCESS**: qwen3-embeddings-mlx works perfectly on Apple Silicon
  - MLX benchmark: 55x faster than Ollama (60ms vs 3300ms for 32 texts with 8B model)
  - Update recommendations: Use MLX for embedding acceleration on Apple Silicon
  - Add MLX installation instructions and API documentation
  - Add comprehensive benchmark comparison tables

---

## Files Changed

### New Files
- `internal/embed/mlx.go` - MLXEmbedder implementation
- `internal/embed/mlx_test.go` - MLX unit tests
- `docs/embedding_optimization_analysis.md` - MLX research and benchmarks

### Modified Files
- `internal/embed/factory.go` - Add ProviderMLX, SetMLXConfig, auto-detection
- `internal/embed/factory_test.go` - Add MLX config tests
- `internal/config/config.go` - Add MLX config fields
- `internal/config/config_test.go` - Update for new defaults
- `cmd/amanmcp/cmd/index.go` - Wire SetMLXConfig
- `cmd/amanmcp/cmd/serve.go` - Wire SetMLXConfig
- `cmd/amanmcp/cmd/search.go` - Wire SetMLXConfig
- `internal/daemon/daemon.go` - Wire SetMLXConfig

---

## Usage

### Auto-detection (Recommended)
```bash
# MLX auto-detects on Apple Silicon when server is running
amanmcp index
```

### Explicit MLX Selection
```bash
AMANMCP_EMBEDDER=mlx amanmcp index
```

### Config File
```yaml
# config.yaml
embeddings:
  provider: "mlx"
  mlx_endpoint: "http://localhost:8000"
  mlx_model: "large"  # small, medium, or large
```

### MLX Server Setup
```bash
# Clone and run MLX server
git clone https://github.com/jakedahn/qwen3-embeddings-mlx
cd qwen3-embeddings-mlx
pip install -r requirements.txt
python server.py  # Runs on http://localhost:8000
```
