# v0.2.4

**Release Date:** 2026-01-12

---

## Added

- **FEAT-DIM2**: Add `amanmcp index info` command to display index configuration, embedding model, dimensions, and statistics. Supports `--json` flag for machine-readable output. Shows compatibility status when current embedder differs from index.

- **FEAT-OBS1 - Indexing Observability**: Enhanced indexing completion output with detailed stage breakdown (scan, chunk, context, embed, index) including timing, throughput (chunks/sec), and backend info. JSON logs now include per-stage millisecond timings for analysis.

  Example output:
  ```
  Complete: 806 files, 13131 chunks indexed in 6m24.3s

  Stage Breakdown:
    Scan:    300ms (files discovered)
    Chunk:   500ms (code parsed)
    Context: 0s (CR-1 enrichment)
    Embed:   5m50s (13131 chunks @ 37.5/sec)
    Index:   33.1s (BM25 + vector)

  Backend: mlx (mlx-qwen3-embedding-small, 1024 dims)
  ```

- **Benchmark Script**: Add `scripts/benchmark-backends.sh` for comparing MLX vs Ollama backend performance with multiple runs and summary tables.

- **FEAT-MLX2 - MLX One-Command Install**: Add `make install-mlx` that sets up Python venv, installs dependencies, and prompts to download Qwen3-0.6B model with progress bar. Also adds `make start-mlx` for easy server startup.

  ```bash
  make install-mlx  # One command: venv + deps + model download
  make start-mlx    # Start MLX server
  ```

## Changed

- **FEAT-BE1 - Ollama Default Backend**: Ollama is now the default embedding backend (previously MLX on Apple Silicon). MLX remains available via `--backend=mlx` flag or `AMANMCP_EMBEDDER=mlx` environment variable.

  **Decision rationale**: Benchmark showed only ~1.7x speed difference (not 55x as previously claimed). Cross-platform benefits outweigh marginal speed gain.

  | Metric | MLX | Ollama |
  |--------|-----|--------|
  | Throughput | 39 chunks/sec | 23.4 chunks/sec |
  | Wall-clock | 6m10s | 6m51s |
  | Platform | Apple Silicon only | Cross-platform |

- **Backend Flag**: Add `--backend` flag to `amanmcp index` command for easy backend switching (ollama, mlx, or static).

  ```bash
  amanmcp index .                    # Uses Ollama (default)
  amanmcp index --backend=mlx .      # Uses MLX
  AMANMCP_EMBEDDER=mlx amanmcp index .  # Via env var
  ```

## Fixed

- **BUG-042**: CLI index now stores embedding dimension and model metadata. This enables dimension mismatch detection when embedder changes between index and search operations, preventing silent incorrect results.

## Documentation

- **SPIKE-003**: Complete 4-track embedding model benchmark. Qwen3-0.6B confirmed as best model (83% Tier 1 vs 75% for EmbeddingGemma). Backend (MLX vs Ollama) doesn't affect quality. MRL dimension testing skipped (EmbeddingGemma quality insufficient). Results in `.aman-pm/validation/embedder-matrix.md`.

## Learnings

Key insights from Sprint 4 development:

1. **MLX is 1.67x faster, NOT 55x** - The "55x faster" claim was never validated. Actual benchmarks show MLX at 39 chunks/sec vs Ollama at 23.4 chunks/sec (1.67x). Wall-clock difference is only ~11% (6m10s vs 6m51s).

2. **Model switching requires clean reindex** - Switching embedding models without `rm -rf .amanmcp` produces identical (wrong) results because the old vectors remain cached.

3. **Observability enables decisions** - Adding stage timing and structured JSON logging was essential for making data-driven backend decisions. Without observability, claims go unvalidated.

4. **Cross-platform > marginal speed** - For developer tools, "just works everywhere" beats "slightly faster on some platforms." Ollama's cross-platform support outweighs MLX's 1.67x speed advantage.

5. **Lazy engineer UX** - One-command solutions (`make install-mlx`, `--backend` flag) dramatically improve adoption. Multiple manual steps create friction.

---

[Full Changelog](../CHANGELOG.md)
